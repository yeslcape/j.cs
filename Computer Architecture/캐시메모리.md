# 캐시메모리

### 개념
컴퓨터 시스템의 성능을 향상시키기 위해 별도로 탑재된 캐시 전용 메모리

### 목적
중앙처리장치(CPU)와 주기억장치(메모리) 간의 속도 차이 개선

### 특징
* CPU와 주 메모리(메인 메모리) 사이에 위치하여 데이터 엑세스 속도를 향상시키고, 시스템 성능 개선
* L1 캐시부터 먼저 사용됨 (CPU에서 가장 빠르게 접근하고, 여기서 데이터를 찾지 못하면 L2로 감)
* 데이터의 지역성 원리를 기반으로 동작
  * 데이터에 접근하는 패턴은 대부분 특정 부분에 집중되는 경향 있음. 
  * 캐시는 이러한 지역성을 활용하여 CPU가 빈번하게 엑세스하는 데이터 저장 
  * -> 이를 통해 CPU는 더 빠른 속도로 데이터에 접근 가능
* 캐시 일관성 프로토콜
  * 캐시 간 데이터 일관성 유지
  * CPU가 변경한 데이터나 다른 캐시나 주 메모리에 올바르게 반영되도록 함
* 프리페치
  * CPU의 명령을 처리하는 동안 데이터를 미리 읽어와 저장
  * 데이터 엑세스 지연을 최소화하고 효율적인 연산 수행 가능

### 캐시 레벨
[캐시영역이 나눠져 있는 이유 : 성능과 비용의 균형을 유지하기 위해서]
* L1 캐시
  * private cache : 특정 코어만을 위해 사용되는 캐시
  * DL1 캐시와 IL1 캐시로 물리적으로 나뉘어져 있음
    * DL1 캐시 : 데이터 저장
    * IL1 캐시 : 명령어 저장
  * CPU 코어에 가장 가까운 위치 (CPU 내부에 존재)
  * 용량 작음
  * 매우 빠른 엑세스 속도
* L2 캐시
  * CPU와 RAM 사이에 존재
  * L2 캐시는 L1 캐시의 다음 계층에 위치
  * 용량 L1보다 큼
  * 엑세스 속도 상대적으로 느림
* L3 캐시
  * L3 캐시는 주로 멀티코어 프로세서에서 사용
  * 용량 큼
  * 엑세스 속도 가장 느림

### 캐시 메모리 사용 이유
1. 데이터 엑세스의 지역성(Locality): 프로그램이나 알고리즘이 특정 데이터를 반복적으로 엑세스 하는 경우, 해당 데이터를 캐시에 저장하면 반복적인 엑세스 시에 주 메모리까지 접근할 필요가 없어지므로 성능 향상 기대할 수 있음
2. 데이터 엑세스 속도의 차이: 주 메모리와 비교하여 상대적으로 접근 시간이 빠른 캐시를 사용함으로써 데이터 엑세스 속도를 향상 시킬 수 있음
3. 데이터의 변경 주기가 빈번하지 않고, 단위 처리 시간이 오래걸리는 경우
4. 데이터의 최신화가 반드시 실시간으로 이루어지지 않아도 서비스 품질에 거의 영향을 주지 않는 데이터

### 캐시 메모리 작동 원리
* 시간 지역성
  * for나 while 같은 반복문에 사용하는 조건 변수처럼 한번 참조된 데이터는 잠시후 또 참조될 가능성이 높음
* 공간 지역성 
  * A[0], A[1]과 같은 연속 접근 시, 참조된 데이터 근처에 있는 데이터가 잠시후 또 사용될 가능성이 높음

### 캐시 메모리 내부 구조
![](https://github.com/yeslcape/schedule/assets/45252618/91597e8c-a69a-4e7a-83f5-cb5a362ffaa7)
* 캐시 : S개의 집합 
* 집합 : E개의 캐시 라인 저장
* 캐시라인 : 메인 메모리에서 가져오는 블록 하나 + 유효 비트 + 태그(동일한 집합에 들어올 수 있는 다른 블록들과 구별하기 위함)
* 캐시 메모리의 총 사이즈 : S * B * E (바이트)

### 캐시 메모리 관련 용어
* Block(또는 (cache) line) : 캐시 메모리와 메인 메모리 사이 주고받는 데이터의 단위(일반적으로 64byte)
* Hit : CPU가 읽어오려고 하는 데이터가 캐시에 있는 경우
  * Hit rate : CPU가 요청한 데이터 중 캐시에 저장된 비율
  * Hit time : 캐시에서 데이터를 읽어오는 데 필요한 시간
* Miss : CPU가 읽어오려고 하는 데이터가 캐시에 없는 경우
  * Miss rate : CPU가 요청한 데이터 중 캐시에 저장되지 않은 비율 (= 1 - Hit rate)
  * Miss penalty : miss가 발생해 데이터를 block만큼 메인 메모리에서 캐시 메모리로 가져오는 데 필요한 시간

### 캐시 미스 경우 3가지
1. Cold miss 
   * 해당 메모리 주소를 처음 불러서 나는 미스
   * cache가 비워져있어서 발생함
2. Conflict miss
   * 캐시 메모리에 A와 B 데이터를 저장해야 하는데, A와 B가 같은 캐시 메모리 주소에 할당되어 있어서 나는 미스 (direct mapped cache에서 많이 발생)
   * A 데이터를 저장한 후, B 데이터가 같은 캐시 메모리 주소에 할당되어 저장되면 이후에 A 데이터를 읽어오려고 했을 때 miss 발생
3. Capacity miss 
   * 캐시 메모리의 공간이 부족해서 나는 미스 (Conflict는 주소 할당 문제, Capacity는 공간 문제)

### 캐시 메모리 구조 및 작동 방식
1. Direct mapped Cache
   ![](https://github.com/yeslcape/schedule/assets/45252618/162dbcc2-9de7-4908-bc1e-a7e6c0997b96)
    * 가장 기본적인 캐시 메모리 구조
    * 각 memory block이 cache의 정확히 한 block에만 대응
      * memory의 00001에 위치한 block은 cache memory의 001에만 저장 가능
      * hit인지 miss인지 판단하기 위해 한 개의 cache block만 확인하면 됨
    * memory block이 저장될 cache block의 주소 : (memory block 주소) % (cache의 block 수)
    * tag : memory block 주소를 cache block 수로 나눈 몫
        * memory block 주소의 상위 두 자리
          * cf) memory block : 여러 개의 memory block은 하나의 cache block 공유
2. Fully associative Cache
   ![](https://github.com/yeslcape/schedule/assets/45252618/cd9e551e-651d-4261-853b-0e7b34878361)
    * direct mapped cache와 정반대 개념
    * memory block은 캐시 메모리의 비어있는 모든 block에 저장될 수 있음
    * hit인지 miss인지 판단하기 위해서는 cache의 모든 block 확인 필요
3. N-way set associative Cache
   ![](https://github.com/yeslcape/schedule/assets/45252618/921b664e-ea8c-4962-9584-d46ae4ece069)
    * 캐시 메모리를 n개의 block(way)를 가진 set이라는 단위로 나누고, memory block을 하나의 set에서 비어있는 block에 저장
    * N이 2라고 가정
      * memory block은 하나의 set에서 2개의 block 중 하나에 저장될 수 있으므로 hit인지 miss인지 판단하기 위해서는 block 2개를 모두 확인 필요
      * (memory block 주소) % (캐시의 set 수)를 계산하여 어느 set를 확인해야하는지 계산 가능



<br><br><br>
Reference
- https://namu.wiki/w/캐시%20메모리
- https://velog.io/@ajm0718/메모리-구조
- https://velog.io/@letskuku/컴퓨터구조-캐시-메모리
- https://github.com/gyoogle/tech-interview-for-developer/blob/master/Computer%20Science/Computer%20Architecture/캐시%20메모리(Cache%20Memory).md
- https://woozzang.tistory.com/155